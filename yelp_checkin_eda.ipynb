{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Imports <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center> Import check_in data from Yelp dataset <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/Users/Lisa/Downloads/yelp_dataset/yelp_academic_dataset_checkin.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkins = pd.read_json(file, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center> Since the date column is a long string I'll need to change it to a list in order to leverage date/time for future graphing <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkins['date'] = checkins['date'].apply(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center> Leverage dataframe.explode('column_name') to break the list of check-in's down <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkins_broken_down = checkins.explode('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center> From my EDA is seems like each business has a unique business_id which gives a breakdown of the dates people checked in to these businesses. I can use that to compare against the weather for those dates. <br/> Now Identify the locations with enough data points to scrap weather data for <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center> Import Business data from Yelp dataset <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = '/Users/Lisa/Downloads/yelp_dataset/yelp_academic_dataset_business.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses = pd.read_json(file2, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center> Clean up businesses dataframe and elimnate unnecessary columns since its not data I need to focus on <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "businesses = businesses.drop(['hours', 'categories', 'attributes', 'longitude', 'latitude', 'stars', 'is_open', 'address', 'postal_code', 'review_count'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center> Idnetify a State with enough data points to work with (since there's a lot of entries) <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses.groupby('state').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>It seems like Arizona has the most amount of Yelp Activity <br/> Determine which city to focus on in AZ. <br/> Groupby and sort the counted data<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZ = businesses.loc[businesses['state'] == 'AZ'].groupby('city').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Take the grouped data and sort it<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZ.sort_values('state', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center> Through EDA, it seems like Phoenix has the greatest activity to measure against weather. Will focus on Yelp check-in's there<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center> Pull Pheonix, AZ from the businesses dataframe <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phx_az_biz = businesses.loc[(businesses['city']=='Phoenix') & (businesses['state']=='AZ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center> Merge Phoenix, AZ dataframe to checkins_broken_down <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phx_checkins = phx_az_biz.merge(checkins_broken_down, how='left', on='business_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center> get date/time to a workable data type (not a string!) <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Example: data[\"Date\"]= pd.to_datetime(data[\"Date\"]) <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phx_checkins['date'] = pd.to_datetime(phx_checkins['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phx_checkins['date2.0'] = phx_checkins['date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phx_checkins = phx_checkins.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center> Groupby and count dates <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center> Break up phx_checkins by years <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phx_checkins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see how to access date/time\n",
    "print(phx_checkins.iloc[0,4].month)\n",
    "print(phx_checkins.iloc[0,4].day)\n",
    "print(phx_checkins.iloc[0,4].year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new columns for month, date, year\n",
    "phx_checkins['Month'] = phx_checkins['date2.0'].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phx_checkins['Day'] = phx_checkins['date2.0'].apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phx_checkins['Year'] = phx_checkins['date2.0'].apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phx_checkins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center> Get a breakdown by year and the total amount of checkins. Next will be to separate them into years <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phx_checkins_years = phx_checkins.groupby('Year').count()\n",
    "phx_checkins_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkins_2019 = phx_checkins.loc[phx_checkins['Year'] == 2019]\n",
    "checkins_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center> Now that I have 2019 checkin data, break it down by months for plotting<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a count of businesses per day\n",
    "# phx_az_biz = businesses.loc[(businesses['city']=='Phoenix') & (businesses['state']=='AZ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = checkins_2019.loc[ (checkins_2019['Day'] == 1) & (checkins_2019['Month'] == 1)]\n",
    "test = test.groupby('business_id').count()\n",
    "# test = test.reset_index()\n",
    "# test = test['business_id'].unique()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't sound like I'll need to leverage the amount of restaraunts that were being check in to, onlt the total per day\n",
    "'''\n",
    "def rest_per_day_jan(df):\n",
    "    lst = []\n",
    "    a = np.arange(1,32)\n",
    "    for date in a:\n",
    "        total = df.loc[ (df['Day'] == date) & (df['Month'] == 1)]\n",
    "        total = total.groupby('business_id').count()\n",
    "        lst.append(len(total))\n",
    "    return lst\n",
    "\n",
    "rest_per_day_jan(checkins_2019)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(x):\n",
    "    x = x['Day'].sort_values()\n",
    "    x = x.to_frame()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_checkins_2019 = checkins_2019[checkins_2019['Month'] == 1]\n",
    "jan_hist = create_df(jan_checkins_2019)\n",
    "jan_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each months data points\n",
    "feb_checkins_2019 = checkins_2019[checkins_2019['Month'] == 2]\n",
    "# feb_hist = create_df(feb_checkins_2019)\n",
    "mar_checkins_2019 = checkins_2019[checkins_2019['Month'] == 3]\n",
    "# mar_hist = create_df(march_checkins_2019)\n",
    "apr_checkins_2019 = checkins_2019[checkins_2019['Month'] == 4]\n",
    "# apr_hist = create_df(april_checkins_2019)\n",
    "may_checkins_2019 = checkins_2019[checkins_2019['Month'] == 5]\n",
    "# may_hist = create_df(may_checkins_2019)\n",
    "jun_checkins_2019 = checkins_2019[checkins_2019['Month'] == 6]\n",
    "# jun_hist = create_df(june_checkins_2019)\n",
    "jul_checkins_2019 = checkins_2019[checkins_2019['Month'] == 7]\n",
    "# jul_hist = create_df(july_checkins_2019)\n",
    "aug_checkins_2019 = checkins_2019[checkins_2019['Month'] == 8]\n",
    "# aug_hist = create_df(august_checkins_2019)\n",
    "sep_checkins_2019 = checkins_2019[checkins_2019['Month'] == 9]\n",
    "# sep_hist = create_df(sept_checkins_2019)\n",
    "oct_checkins_2019 = checkins_2019[checkins_2019['Month'] == 10]\n",
    "# oct_hist = create_df(oct_checkins_2019)\n",
    "nov_checkins_2019 = checkins_2019[checkins_2019['Month'] == 11]\n",
    "# nov_hist = create_df(nov_checkins_2019)\n",
    "dec_checkins_2019 = checkins_2019[checkins_2019['Month'] == 12]\n",
    "# dec_hist = create_df(dec_checkins_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Test out graphing by month <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = jan_checkins_2019['Day'].unique()\n",
    "x_ticks = np.sort(x_ticks)\n",
    "x_ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8), sharex=True, sharey=True)\n",
    "ax.hist(x = jan_checkins_2019['Day'], bins=np.arange(1,32)-1, align='left')\n",
    "#ax.hist(x = jan_checkins_2019['Day'], bins=x_ticks, align='mid')\n",
    "plt.xticks(x_ticks)\n",
    "plt.tight_layout()\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Total Checkins')\n",
    "ax.title.set_text(\"January 2019\")\n",
    "# ax.plot(rest_per_day_jan(checkins_2019),color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET checkins DF cleaned up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to drop\n",
    "drop = ['business_id', 'city', 'state', 'date2.0', 'Month', 'Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jan = jan_checkins_2019.groupby('Day').count()\n",
    "jan = jan.reset_index()\n",
    "jan.drop(drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feb = feb_checkins_2019.groupby('Day').count()\n",
    "feb = feb.reset_index()\n",
    "feb.drop(drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mar = mar_checkins_2019.groupby('Day').count()\n",
    "mar = mar.reset_index()\n",
    "mar.drop(drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apr = apr_checkins_2019.groupby('Day').count()\n",
    "apr = apr.reset_index()\n",
    "apr.drop(drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "may = may_checkins_2019.groupby('Day').count()\n",
    "may = may.reset_index()\n",
    "may.drop(drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jun = jun_checkins_2019.groupby('Day').count()\n",
    "jun = jun.reset_index()\n",
    "jun.drop(drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly weather data imports\n",
    "jan2 = pd.read_csv('/Users/Lisa/capstone_1/jan.csv')\n",
    "feb2 = pd.read_csv('/Users/Lisa/capstone_1/feb.csv')\n",
    "mar2 = pd.read_csv('/Users/Lisa/capstone_1/mar.csv')\n",
    "apr2 = pd.read_csv('/Users/Lisa/capstone_1/apr.csv')\n",
    "may2 = pd.read_csv('/Users/Lisa/capstone_1/may.csv')\n",
    "jun2 = pd.read_csv('/Users/Lisa/capstone_1/jun.csv')\n",
    "# jul2 = pd.read_csv('/Users/Lisa/capstone_1/jul.csv')\n",
    "# aug2 = pd.read_csv('/Users/Lisa/capstone_1/aug.csv')\n",
    "# sep2 = pd.read_csv('/Users/Lisa/capstone_1/sep.csv')\n",
    "# _oct2 = pd.read_csv('/Users/Lisa/capstone_1/oct.csv')\n",
    "# nov2 = pd.read_csv('/Users/Lisa/capstone_1/nov.csv')\n",
    "# dec2 = pd.read_csv('/Users/Lisa/capstone_1/dec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jan = jan.merge(jan, how='left', on='Day')\n",
    "# jan.drop(drop, axis=1)\n",
    "jan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
